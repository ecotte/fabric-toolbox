{
  "queryset": {
    "version": "1.0.0",
    "dataSources": [
      {
        "id": "0c2b24ea-e5a6-4ff8-9760-c23d5cb16d56",
        "clusterUri": "https://trd-70eqys55xv9uvxyttq.z0.kusto.fabric.microsoft.com",
        "type": "Fabric",
        "databaseItemId": "1a4258e0-e906-44d5-857e-8f2a71df6484",
        "databaseItemName": "Spark Monitoring"
      }
    ],
    "tabs": [
      {
        "id": "6e34d028-cfb0-479b-bcc5-25ea42de7011",
        "content": "ApplicationSumma\n\n\n\t",
        "title": "Tab",
        "dataSourceId": "0c2b24ea-e5a6-4ff8-9760-c23d5cb16d56"
      },
      {
        "id": "c73abb80-0632-49fe-bc38-266e9a10b8a0",
        "content": "RawLogs\n| take 10\n\nRawLogs\n| where timestamp <= ago(1m)\n| extend props = parse_json(properties)\n| where applicationId != \"\"\n| project\n    EventTime=timestamp,\n    EventType=\"\",\n    fabricLivyId,\n    capacityId,\n    AppId=applicationId,\n    AppName=applicationName,\n    User=userId,\n    JobId='EMPTY',\n    StageID=toint(props.[\"Stage ID\"]),\n    StageAttemptId=toint(props.[\"Stage Attempt ID\"]),\n    TaskId=toint(props.[\"Task Info\"].[\"Task ID\"]),\n    taskAttemptId=toint(props.[\"Task Info\"].[\"Attempt\"]),\n    executorId=executorId,\n    Host='NOTPRESENT',\n    DurationMs='INSIDE-MetricsJson',\n    Status=props.[\"Task End Reason\"],\n    FailureReason=props.[\"Task End Reason\"].[\"Reason\"],\n    MetricsJson=props.[\"Task Metrics\"],\n    EventDetailsJson=properties\n| summarize count_distinct(executorId) by AppId, fabricLivyId, capacityId, bin(EventTime, 1m)\n| sort by count_distinct_executorId desc\n\n\nRawLogs\n| where applicationId !in (sparklens_metadata | project applicationId)\n| where category == \"EventLog\"\n| where isnotempty(properties)\n",
        "title": "",
        "dataSourceId": "0c2b24ea-e5a6-4ff8-9760-c23d5cb16d56"
      },
      {
        "id": "d6592d0a-2a4f-446b-a937-bd0fdc853660",
        "content": "sparklens_metadata \n| project applicationId\n| distinct applicationId",
        "title": "",
        "dataSourceId": "0c2b24ea-e5a6-4ff8-9760-c23d5cb16d56"
      },
      {
        "id": "50ba8099-c144-401b-8502-887739f0ad27",
        "content": "\n\n.show table sparklens_summary schema as json",
        "title": "",
        "dataSourceId": "0c2b24ea-e5a6-4ff8-9760-c23d5cb16d56"
      },
      {
        "id": "f877aa88-db2e-4f10-bf8f-df77a3186bbc",
        "content": "\nRawLogs\n| where isnotempty(applicationId)\n| distinct applicationId, applicationName, fabricLivyId\n\n\nlet _sparkApplication = RawLogs\n| where isnotempty(applicationId)\n| distinct applicationId, applicationName, fabricLivyId=tostring(fabricLivyId);\nlet _operations = cluster(\"https://trd-udaj8fq7435z2jsjbg.z9.kusto.fabric.microsoft.com\").database(\"Capacity Utilization\").CapacityOperations;\n_sparkApplication\n| join _operations on $left.fabricLivyId == $right.operationId\n| summarize CU=sum(capacityUnitMs)/1000 by applicationName, applicationId, fabricLivyId\n\n\n\n\nRawLogs\n| where category == \"Log\"\n|take 1000\n\n\nRawLogs\n| distinct category\n\n\nRawLogsWSM\n| take 100\n\n",
        "title": "",
        "dataSourceId": "0c2b24ea-e5a6-4ff8-9760-c23d5cb16d56"
      },
      {
        "id": "4ba7f380-424d-4741-8642-61517897067f",
        "content": ".create-or-alter function with (skipvalidation = \"true\") Update_SparkEventLogs() {    \n     RawLogs\n    | where records[\"category\"] == \"EventLog\"\n    | project\n        Timestamp=todatetime(records[\"timestamp\"]),\n        OperationName=tostring(records[\"category\"]),\n        ItemId=toguid(records[\"artifactId\"]),\n        ItemKind=tostring(records[\"artifactType\"]),\n        ItemName=tostring(\"\"),\n        WorkspaceId=toguid(records[\"fabricWorkspaceId\"]),\n        WorkspaceName=tostring(\"\"),\n        CapacityId=toguid(records[\"capacityId\"]),\n        CapacityName=tostring(\"\"),\t\n        CorrelationId=toguid(records[\"fabricLivyId\"]),\n        OperationId=toguid(records[\"fabricLivyId\"]),\n        Identity= bag_pack(\"UseId\", tostring(records[\"userId\"])),\t\n        CustomerTenantId=toguid(records[\"fabricTenantId\"]),\n        DurationMs=iff(records[\"properties\"].[\"Event\"] == \"SparkListenerTaskEnd\", tolong(records[\"properties\"].[\"Task Info\"].[\"Finish Time\"]) - tolong(records[\"properties\"].[\"Task Info\"].[\"Launch Time\"]), long(null)),\n        Status=tostring(records[\"properties\"].[\"Task End Reason\"].[\"Reason\"]),\n        Level=tostring(\"\"),\n        Region=tostring(\"\"),\n        Category=tostring(records[\"category\"]),\n        WorkspaceMonitoringTable=tostring(\"SparkEventLogs\"),\n        //End of Common columns\n        LogTime=todatetime(records[\"timestamp\"]),\n        EventType=tostring(records[\"properties\"].[\"Event\"]),\n        AppId=tostring(records[\"applicationId\"]),\n        AppName=tostring(records[\"applicationName\"]),\n        User=toguid(records[\"userId\"]),\n        JobId=toint(records[\"properties\"].[\"Job ID\"]),\n        StageId=toint(records[\"properties\"].[\"Stage ID\"]),\n        StageAttemptId=toint(records[\"properties\"].[\"Stage Attempt ID\"]),\n        TaskId=toint(records[\"properties\"].[\"Task Info\"].[\"Task ID\"]),\n        TaskAttempt=toint(records[\"properties\"].[\"Task Info\"].[\"Attempt\"]),\n        ExecutorId=tostring(records[\"executorId\"]),\n        Host=tostring(records[\"properties\"].[\"Task Info\"].[\"Host\"]),\n        FailureReason= iff(tostring(records[\"properties\"].[\"Task End Reason\"].[\"Reason\"]) == \"Success\", \"\", tostring(records[\"properties\"].[\"Task End Reason\"])),\n        MetricsJson=todynamic(records[\"properties\"].[\"Task Metrics\"]),\n        PropertiesJson=todynamic(records[\"properties\"]),\n        EventDetailsJson=todynamic(records)\n }\n\n\n\n.alter table SparkEventLogs policy update \"[{\\\"IsEnabled\\\":true,\\\"Source\\\":\\\"RawLogs\\\",\\\"Query\\\":\\\"Update_SparkEventLogs\\\",\\\"IsTransactional\\\":false,\\\"PropagateIngestionProperties\\\":true,\\\"ManagedIdentity\\\":null}]\"\n\n\n",
        "title": "",
        "dataSourceId": "0c2b24ea-e5a6-4ff8-9760-c23d5cb16d56"
      },
      {
        "id": "89fb0e0d-68c9-4e25-898e-3b0458e45480",
        "content": ".create-or-alter function with (skipvalidation = \"true\") Update_SparkDriverLogs() {\n     RawLogs\n    | where records[\"category\"] in (\"Log\")\n    | where records[\"executorId\"] == \"driver\"\n    | project\n    Timestamp=todatetime(records[\"timestamp\"]),\n        OperationName=tostring(records[\"category\"]),\n        ItemId=toguid(records[\"artifactId\"]),\n        ItemKind=tostring(records[\"artifactType\"]),\n        ItemName=tostring(\"\"),\n        WorkspaceId=toguid(records[\"fabricWorkspaceId\"]),\n        WorkspaceName=tostring(\"\"),\n        CapacityId=toguid(records[\"capacityId\"]),\n        CapacityName=tostring(\"\"),\t\n        CorrelationId=toguid(records[\"fabricLivyId\"]),\n        OperationId=toguid(records[\"fabricLivyId\"]),\n        Identity= bag_pack(\"UseId\", tostring(records[\"userId\"])),\t\n        CustomerTenantId=toguid(records[\"fabricTenantId\"]),\n        Status=tostring(records[\"properties\"].[\"Task End Reason\"].[\"Reason\"]),\n        Level=tostring(records[\"properties\"].[\"level\"]),\n        Region=tostring(\"\"),\n        Category=tostring(records[\"category\"]),\n        WorkspaceMonitoringTable=tostring(\"SparkDriverLogs\"),\n        //End Common Columns\n        LogTime=todatetime(records[\"timestamp\"]),\n        AppId=tostring(records[\"applicationId\"]),\n        AppName=tostring(records[\"applicationName\"]),\n        DriverId=tostring(records[\"executorId\"]),\n        LogLevel=tostring(records[\"properties\"].[\"level\"]),\n        LoggerName=tostring(records[\"properties\"].[\"logger_name\"]),\n        Thread=tostring(records[\"properties\"].[\"thread_name\"]),\n        Message=tostring(records[\"properties\"].[\"message\"]),\n        Exception=todynamic(records[\"properties\"].[\"exception\"]),\n        Hostname=tostring(\"\"),\n        LogSource=tostring(\"\")\n }\n\n\n.alter table SparkDriverLogs policy update \"[{\\\"IsEnabled\\\":true,\\\"Source\\\":\\\"RawLogs\\\",\\\"Query\\\":\\\"Update_SparkDriverLogs\\\",\\\"IsTransactional\\\":false,\\\"PropagateIngestionProperties\\\":true,\\\"ManagedIdentity\\\":null}]\"\n\n\nSparkDriverLogs\n| take 10",
        "title": "",
        "dataSourceId": "0c2b24ea-e5a6-4ff8-9760-c23d5cb16d56"
      },
      {
        "id": "45a19b7f-aace-41ca-b74f-0b144b53966b",
        "content": ".create-or-alter function with (skipvalidation = \"true\") Update_SparkExecutorLogs() {\n    RawLogs\n    | where records[\"category\"] in (\"Log\")\n    | where records[\"executorId\"] != \"driver\"\n    | project\n        Timestamp=todatetime(records[\"timestamp\"]),\n        OperationName=tostring(records[\"category\"]),\n        ItemId=toguid(records[\"artifactId\"]),\n        ItemKind=tostring(records[\"artifactType\"]),\n        ItemName=tostring(\"\"),\n        WorkspaceId=toguid(records[\"fabricWorkspaceId\"]),\n        WorkspaceName=tostring(\"\"),\n        CapacityId=toguid(records[\"capacityId\"]),\n        CapacityName=tostring(\"\"),\t\n        CorrelationId=toguid(records[\"fabricLivyId\"]),\n        OperationId=toguid(records[\"fabricLivyId\"]),\n        Identity= bag_pack(\"UseId\", tostring(records[\"userId\"])),\t\n        CustomerTenantId=toguid(records[\"fabricTenantId\"]),\n        Status=tostring(records[\"properties\"].[\"Task End Reason\"].[\"Reason\"]),\n        Level=tostring(records[\"properties\"].[\"level\"]),\n        Region=tostring(\"\"),\n        Category=tostring(records[\"category\"]),\n        WorkspaceMonitoringTable=tostring(\"SparkExecutorLogs\"),\n        //End Common Columns\n        LogTime=todatetime(records[\"timestamp\"]),\n        AppId=tostring(records[\"applicationId\"]),\n        AppName=tostring(records[\"applicationName\"]),\n        ExecutorId=tostring(records[\"executorId\"]),\n        Host=tostring(\"\"),\n        LogLevel=tostring(records[\"properties\"].[\"level\"]),\n        LoggerName=tostring(records[\"properties\"].[\"logger_name\"]),\n        Thread=tostring(records[\"properties\"].[\"thread_name\"]),\n        Message=tostring(records[\"properties\"].[\"message\"]),\n        Exception=todynamic(records[\"properties\"].[\"exception\"]),\n        TaskId=tostring(\"\"),\n        StageId=tostring(\"\"),\n        LogSource=tostring(\"\")\n }\n\n\n",
        "title": "",
        "dataSourceId": "0c2b24ea-e5a6-4ff8-9760-c23d5cb16d56"
      },
      {
        "id": "37345ae5-55ae-4a80-a085-04285ecb3586",
        "content": ".create-or-alter function with (skipvalidation = \"true\") Update_SparkSQLExecutionEvents() {    \n     RawLogs\n    | where records[\"properties\"].[\"Event\"] in (\"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart\", \"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionPhaseUpdates\", \"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd\")\n    | project\n        Timestamp=todatetime(records[\"timestamp\"]),\n        OperationName=tostring(records[\"category\"]),\n        ItemId=toguid(records[\"artifactId\"]),\n        ItemKind=tostring(records[\"artifactType\"]),\n        ItemName=tostring(\"\"),\n        WorkspaceId=toguid(records[\"fabricWorkspaceId\"]),\n        WorkspaceName=tostring(\"\"),\n        CapacityId=toguid(records[\"capacityId\"]),\n        CapacityName=tostring(\"\"),\t\n        CorrelationId=toguid(records[\"fabricLivyId\"]),\n        OperationId=toguid(records[\"fabricLivyId\"]),\n        Identity= bag_pack(\"UseId\", tostring(records[\"userId\"])),\t\n        CustomerTenantId=toguid(records[\"fabricTenantId\"]),\n        Status=tostring(records[\"properties\"].[\"Task End Reason\"].[\"Reason\"]),\n        Level=tostring(records[\"properties\"].[\"level\"]),\n        Region=tostring(\"\"),\n        Category=tostring(records[\"category\"]),\n        WorkspaceMonitoringTable=tostring(\"SparkSQLExecutionEvents\"),\n        //End Common Columns\n        EventType=tostring(records[\"properties\"].[\"Event\"]),\n        AppId = tostring(records[\"applicationId\"]),\n        AppName = tostring(records[\"applicationName\"]),\n        ExecutorId = tostring(records[\"executorId\"]),\n        StartTime = iff(records[\"properties\"].[\"Event\"] == \"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart\", unixtime_milliseconds_todatetime(tolong(records[\"properties\"].[\"time\"])), datetime(null)),\n        EndTime=iff(records[\"properties\"].[\"Event\"] == \"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd\", unixtime_milliseconds_todatetime(tolong(records[\"properties\"].[\"time\"])), datetime(null)),\n        PhysicalPlanDescription=tostring(records[\"properties\"].[\"physicalPlanDescription\"]),\n        SparkPlanInfo=todynamic(records[\"properties\"].[\"SparkPlanInfo\"]),\n        Role=\"\",\n        RoleInstance=\"\",\n        PreciseTimeStamp=todatetime(records[\"timestamp\"]),\n        SourceNamespace=\"\",\n        SourceMoniker=\"\",\n        SourceVersion=\"\",\n        AppKind=\"\",\n        SessionType=\"\"\n }\n\n\n.alter table SparkSQLExecutionEvents policy update \"[{\\\"IsEnabled\\\":true,\\\"Source\\\":\\\"RawLogs\\\",\\\"Query\\\":\\\"Update_SparkSQLExecutionEvents\\\",\\\"IsTransactional\\\":false,\\\"PropagateIngestionProperties\\\":true,\\\"ManagedIdentity\\\":null}]\"",
        "title": "",
        "dataSourceId": "0c2b24ea-e5a6-4ff8-9760-c23d5cb16d56"
      },
      {
        "id": "51e86bd3-e3b8-4703-9610-b42e9ad6afcb",
        "content": ".create-or-alter function with (skipvalidation = \"true\") Update_SparkDerivedTaskMetrics() {\nRawLogs\n| where records[\"properties\"].[\"Event\"] == \"SparkListenerTaskEnd\"\n| where toint(records[\"properties\"].[\"Task Info\"].[\"Task ID\"]) > 0\n| extend taskDuration = todouble(records[\"properties\"].[\"Task Info\"].[\"Finish Time\"]) - todouble(records[\"properties\"].[\"Task Info\"].[\"Launch Time\"])\n| project\n    Timestamp=todatetime(records[\"timestamp\"]),\n    OperationName=tostring(records[\"category\"]),\n    ItemId=toguid(records[\"artifactId\"]),\n    ItemKind=tostring(records[\"artifactType\"]),\n    ItemName=tostring(\"\"),\n    WorkspaceId=toguid(records[\"fabricWorkspaceId\"]),\n    WorkspaceName=tostring(\"\"),\n    CapacityId=toguid(records[\"capacityId\"]),\n    CapacityName=tostring(\"\"),\t\n    CorrelationId=toguid(records[\"fabricLivyId\"]),\n    OperationId=toguid(records[\"fabricLivyId\"]),\n    Identity= bag_pack(\"UseId\", tostring(records[\"userId\"])),\t\n    CustomerTenantId=toguid(records[\"fabricTenantId\"]),\n    Status=tostring(records[\"properties\"].[\"Task End Reason\"].[\"Reason\"]),\n    Level=tostring(records[\"properties\"].[\"level\"]),\n    Region=tostring(\"\"),\n    Category=tostring(records[\"category\"]),\n    WorkspaceMonitoringTable=tostring(\"SparkDerivedTaskMetrics\"),\n    //End Common Columns\n    AppId = tostring(records[\"applicationId\"]),\n    AppName = tostring(records[\"applicationName\"]),\n    StageId = toint(records[\"properties\"].[\"Stage ID\"]),\n    TaskId = toint(records[\"properties\"].[\"Task Info\"].[\"Task ID\"]),\n    EfficiencyRatio = 100.0 * todouble(records[\"properties\"].[\"Task Metrics\"].[\"Executor CPU Time\"]) / 1000000 / tolong(records[\"properties\"].[\"Task Metrics\"].[\"Executor Run Time\"]),\n    GcOverheadRatio = 100.0 * todouble(tolong(records[\"properties\"].[\"Task Metrics\"].[\"JVM GC Time\"]) / (taskDuration)),\n    ShuffleOverheadRatio = iff(\n                           todouble(records[\"properties\"].[\"Task Metrics\"][\"Input Metrics\"].[\"Bytes Read\"]) == 0.0,\n                           0.0,\n                           (\n    todouble(records[\"properties\"].[\"Task Metrics\"][\"Shuffle Read Metrics\"].[\"Remote Bytes Read\"]) + \n    todouble(records[\"properties\"].[\"Task Metrics\"][\"Shuffle Read Metrics\"].[\"Local Bytes Read\"]) + \n    todouble(records[\"properties\"].[\"Task Metrics\"][\"Shuffle Write Metrics\"].[\"Shuffle Bytes Written\"])\n    ) / todouble(records[\"properties\"].[\"Task Metrics\"][\"Input Metrics\"].[\"Bytes Read\"])\n                       ),\n    WasSpilled = tolong(records[\"properties\"].[\"Task Metrics\"].[\"Memory Bytes Spilled\"]) + tolong(records[\"properties\"].[\"Task Metrics\"].[\"Disk Bytes Spilled\"]),\n    TaskDurationMs = tolong(taskDuration * 1000)\n}\n\n\n\n.alter table SparkDerivedTaskMetrics policy update \"[{\\\"IsEnabled\\\":true,\\\"Source\\\":\\\"RawLogs\\\",\\\"Query\\\":\\\"Update_SparkDerivedTaskMetrics\\\",\\\"IsTransactional\\\":false,\\\"PropagateIngestionProperties\\\":true,\\\"ManagedIdentity\\\":null}]\"\n",
        "title": "",
        "dataSourceId": "0c2b24ea-e5a6-4ff8-9760-c23d5cb16d56"
      },
      {
        "id": "491158f4-6c54-4915-b1fb-2f74734dabc4",
        "content": ".create-or-alter function with (skipvalidation = \"true\") Update_SparkMetrics() {\nRawLogs\n| where records[\"category\"] == \"Metrics\"\n| extend name = records[\"properties\"].[\"name\"]\n| parse name with applicationId_p: string \".\" process: string \".\" metric: string\n| project \n    Timestamp=todatetime(records[\"timestamp\"]),\n    OperationName=tostring(records[\"category\"]),\n    ItemId=toguid(records[\"artifactId\"]),\n    ItemKind=tostring(records[\"artifactType\"]),\n    ItemName=tostring(\"\"),\n    WorkspaceId=toguid(records[\"fabricWorkspaceId\"]),\n    WorkspaceName=tostring(\"\"),\n    CapacityId=toguid(records[\"capacityId\"]),\n    CapacityName=tostring(\"\"),\t\n    CorrelationId=toguid(records[\"fabricLivyId\"]),\n    OperationId=toguid(records[\"fabricLivyId\"]),\n    Identity= bag_pack(\"UseId\", tostring(records[\"userId\"])),\t\n    CustomerTenantId=toguid(records[\"fabricTenantId\"]),\n    Status=tostring(records[\"properties\"].[\"Task End Reason\"].[\"Reason\"]),\n    Level=tostring(records[\"properties\"].[\"level\"]),\n    Region=tostring(\"\"),\n    Category=tostring(records[\"category\"]),\n    WorkspaceMonitoringTable=tostring(\"SparkMetrics\"),\n    //End Common Columns\n    AppId = tostring(records[\"applicationId\"]),\n    AppName = tostring(records[\"applicationName\"]),\n    ExecutorId = tostring(records[\"executorId\"]),\n    Process = tostring(process),\n    Metric = tostring(metric),\n    MetricType = tostring(records[\"properties\"].[\"metric_type\"]),\n    DurationUnits = tostring(records[\"properties\"].[\"duration_units\"]),\n    RateUnits = tostring(records[\"properties\"].[\"rate_units\"]),\n    Value = toreal(records[\"properties\"].[\"value\"]),\n    Count = tolong(records[\"properties\"].[\"count\"]),    \n    Min = toreal(records[\"properties\"].[\"min\"]),\n    Max = toreal(records[\"properties\"].[\"max\"]),\n    Mean = toreal(records[\"properties\"].[\"mean\"]),\n    StdDev = toreal(records[\"properties\"].[\"stddev\"]),\n    MeanRate = toreal(records[\"properties\"].[\"mean_rate\"]),\n    M1Rate = toreal(records[\"properties\"].[\"m1_rate\"]),\n    M5Rate = toreal(records[\"properties\"].[\"m5_rate\"]),\n    M15Rate = toreal(records[\"properties\"].[\"m15_rate\"]),\n    P50 = toreal(records[\"properties\"].[\"p50\"]),\n    P75 = toreal(records[\"properties\"].[\"p75\"]),\n    P95 = toreal(records[\"properties\"].[\"p95\"]),\n    P98 = toreal(records[\"properties\"].[\"p98\"]),\n    P99 = toreal(records[\"properties\"].[\"p99\"]),\n    P999 = toreal(records[\"properties\"].[\"p999\"])\n}\n\n\n\n\n.alter table SparkMetrics policy update \"[{\\\"IsEnabled\\\":true,\\\"Source\\\":\\\"RawLogs\\\",\\\"Query\\\":\\\"Update_SparkMetrics\\\",\\\"IsTransactional\\\":false,\\\"PropagateIngestionProperties\\\":true,\\\"ManagedIdentity\\\":null}]\"",
        "title": "",
        "dataSourceId": "0c2b24ea-e5a6-4ff8-9760-c23d5cb16d56"
      },
      {
        "id": "428ae69e-9571-41d9-a909-5649d9da93aa",
        "content": "// Use 'take' to view a sample number of records in the table and check the data.\nSparkSQLExecutionEvents \n| project AppId, PhysicalPlanDescription\n",
        "title": "",
        "dataSourceId": "0c2b24ea-e5a6-4ff8-9760-c23d5cb16d56"
      }
    ]
  }
}